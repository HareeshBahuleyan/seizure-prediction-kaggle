{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit \n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the file with the features here\n",
    "base_dir_train = '/home/hareesh/IPython/Seizure_Pred/'\n",
    "df1 = pd.read_csv(base_dir_train+\"New_Features/train_3.csv\")\n",
    "df1.columns = ['file', 'label'] + range(1,232) \n",
    "\n",
    "# Drop rows which are empty and have no features extracted\n",
    "df1.dropna(inplace=True)\n",
    "df1.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove files marked as SAFE = 0 from training set\n",
    "safe_indicator_list = pd.read_csv(\"train_and_test_data_labels_safe.csv\")\n",
    "safe_files_list = list(safe_indicator_list[safe_indicator_list['safe']==1]['image'])\n",
    "safe_index = [i for i in range(len(df1['file'])) if df1.ix[i, 'file'] in safe_files_list] \n",
    "df1 = df1.ix[safe_index,]\n",
    "df1.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "For cross-validation while model fitting, we train.csv to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly select 80 rows of pre-ictal and 200 rows of interictal for training\n",
    "np.random.seed(5)\n",
    "train_ix = list(np.random.choice(df1[df1['label']==1].index, size=80, replace=False))+list(np.random.choice(df1[df1['label']==0].index, size=1000, replace=False))\n",
    "\n",
    "#The remaining rows will serve as the test set\n",
    "test_ix = list(set(df1.index)-set(train_ix))\n",
    "\n",
    "X = df1.ix[:,2:]\n",
    "y = df1.ix[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalizing the entire data to values between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "# Now apply the transformations to the data:\n",
    "X = pd.DataFrame(scaler.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis\n",
    "Since the features extracted had high correlations, use PCA to extract linearly uncorrelated features in the directions of maximum variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(n_components=25)\n",
    "pca.fit(X)\n",
    "X = pd.DataFrame(pca.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Minority Oversampling Technique\n",
    "SMOTE is a oversampling technique that creates fake or synthetic samples in the neighbourhood of the minority class data points. By doing this, we try to address the imbalance in the data (i.e., very few pre-ictals when compared to interictals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All columns except the last has the predictor X's\n",
    "X_subset = X.ix[train_ix,:]\n",
    "\n",
    "# The last column is the class label of 0s and 1s\n",
    "y_subset = y[train_ix]\n",
    "\n",
    "svm_args={'class_weight': 'balanced'}\n",
    "smote = SMOTE(ratio='auto', kind='svm')\n",
    "x_, y_ = smote.fit_sample(X_subset, y_subset)\n",
    "\n",
    "X_train = pd.DataFrame(x_)\n",
    "y_train = pd.Series(y_)\n",
    "\n",
    "X_test =  X.ix[test_ix,:]\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test =  y.ix[test_ix]\n",
    "y_test.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation \n",
    "cv = StratifiedShuffleSplit(y_train, n_iter=5, test_size=0.1, random_state=11)\n",
    "\n",
    "classifier = XGBClassifier(n_estimators = 500, gamma=0, subsample=0.8, max_depth=9,\n",
    "                            objective='binary:logistic',learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search Cross Validation\n",
    "Specify the parameters that you want to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "#     'n_estimators': [200,500],\n",
    "#     'gamma':[0,0.1,1],\n",
    "#     'subsample':[0.8,1.0],\n",
    "#     'max_depth':[9],\n",
    "#    'learning_rate':[0.1,0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "XGB Boost classifier <br>\n",
    "* Tune Parameters\n",
    "* Extract most important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} 0.738397790055\n",
      "{} 0.736187845304\n",
      "{} 0.722586935327\n",
      "{} 0.71523399415\n",
      "{} 0.747960675983\n",
      "Average Accuracy: 0.941829393628\n",
      "Average AUC: 0.732073448164\n",
      "Average Percentage of 1s in prediction: 0.0565262076053\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "auc = []\n",
    "percent_ones = []\n",
    "# most_imp_100_features_xgb = set()\n",
    "for (train, test) in cv:\n",
    "    \n",
    "    CV_xgb = GridSearchCV(estimator=classifier, param_grid=param_grid, cv= 3)\n",
    "    \n",
    "    probas_ = CV_xgb.fit(X_train.ix[train,:], y_train[train]).predict_proba(X_test)\n",
    "    y_pred = CV_xgb.predict(X_test)\n",
    "    print CV_xgb.best_params_, roc_auc_score(y_test, y_pred)        \n",
    "    \n",
    "#     # Feature Importances\n",
    "#     importances = classifier.feature_importances_\n",
    "#     indices = np.argsort(importances)[::-1]\n",
    "#     most_imp_100_features_xgb = most_imp_100_features_gb.union(set(indices[:100]))\n",
    "\n",
    "    accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    auc.append(roc_auc_score(y_test, y_pred))\n",
    "    percent_ones.append(y_pred.mean())\n",
    "    \n",
    "print 'Average Accuracy:', np.mean(accuracy)\n",
    "print 'Average AUC:', np.mean(auc)\n",
    "print 'Average Percentage of 1s in prediction:', np.mean(percent_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the block below to obtain 3 output files with predicted labels (one for each patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set Patient Number\n",
    "num = 3\n",
    "\n",
    "# Read the file with the features here\n",
    "base_dir_train = '/home/hareesh/IPython/Seizure_Pred/New_Features/'\n",
    "df_train = pd.read_csv(base_dir_train+\"train_\"+str(num)+\".csv\", na_values='')\n",
    "df_train.columns = ['file', 'label'] + range(1,232) \n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Remove files marked as SAFE = 0 from training set\n",
    "safe_indicator_list = pd.read_csv(base_dir_train+\"train_and_test_data_labels_safe.csv\")\n",
    "safe_files_list = list(safe_indicator_list[safe_indicator_list['safe']==1]['image'])\n",
    "safe_index = [i for i in range(len(df_train['file'])) if df_train.ix[i, 'file'] in safe_files_list] \n",
    "df_train = df_train.ix[safe_index,]\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_test = pd.read_csv(base_dir_train+\"test_\"+str(num)+\"_new.csv\", na_values='')\n",
    "df_test.columns = ['file'] + range(1,232) \n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "X_test = df_test.ix[:,1:]\n",
    "\n",
    "# All columns except the last has the predictor X's\n",
    "X = pd.concat([df_train.ix[:,2:], df_test.ix[:,1:]],ignore_index=True)\n",
    "\n",
    "#Normalizing the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "# Now apply the transformations to the data:\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "\n",
    "# Do PCA for dimensionality reduction. Experiment with number of principal components.\n",
    "# Comment it out if not required\n",
    "\n",
    "pca = IncrementalPCA(n_components=25)\n",
    "pca.fit(X)\n",
    "X = pd.DataFrame(pca.transform(X))\n",
    "\n",
    "# The last column is the class label of 0s and 1s\n",
    "y_train = df_train.ix[:,1]\n",
    "\n",
    "X_train = X.ix[range(df_train.shape[0]),:]\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "X_test = X.ix[df_train.shape[0]:,:]\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "svm_args={'class_weight': 'balanced'}\n",
    "smote = SMOTE(ratio='auto', kind='svm')\n",
    "x_, y_ = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "X_train = pd.DataFrame(x_)\n",
    "y_train = pd.Series(y_)\n",
    "\n",
    "classifier = XGBClassifier(n_estimators = 500, gamma=0, subsample=0.8, max_depth=9,\n",
    "                             objective='binary:logistic',learning_rate=0.1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame(y_pred, index=df_test['file'], columns=['Class'])\n",
    "output.index.name = 'File'\n",
    "output['Class'] = [p[1] for p in classifier.predict_proba(X_test)]\n",
    "output.to_csv(\"test\"+str(num)+\"_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the submission file as per the competition format: sample_submission.csv <br>\n",
    "Keep test_1_output.csv, test_2_output.csv and test_2_output.csv in the sample folder Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 2)\n",
      "(894, 2)\n",
      "(1888, 2)\n"
     ]
    }
   ],
   "source": [
    "# Combining results into 1 file according to the submission format\n",
    "submission_dir = \"/home/hareesh/IPython/Seizure_Pred/\"\n",
    "submission_file = pd.read_csv(submission_dir + \"sample_submission.csv\")\n",
    "\n",
    "pred_dir = submission_dir + \"Pred/\"\n",
    "test_files = os.listdir(pred_dir)\n",
    "df = pd.DataFrame(columns=['File', 'Class'])\n",
    "for file_name in test_files:\n",
    "        df_1 = pd.read_csv(pred_dir + file_name)\n",
    "        df = pd.concat([df,df_1])\n",
    "        print df.shape\n",
    "submission_file = pd.merge(submission_file, df, how='left', on = ['File'], suffixes=['_1',''])\n",
    "submission_file['Class'] = submission_file['Class'].fillna(0)\n",
    "submission_file.drop(['Class_1'], axis=1, inplace=True)\n",
    "submission_file.to_csv(\"Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
